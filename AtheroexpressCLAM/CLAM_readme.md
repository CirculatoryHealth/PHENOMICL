#PIPELINE



**_STEP 1: SEGMENTATION**_ 

python create_patches_fp.py --source DATA_DIRECTORY --save_dir RESULTS_DIRECTORY --patch_size 256 --seg --patch --stitch

**parameters**

    --source: source folder containing the WSI slides (.tiff)
    --save_dir: directory to store
    --patch_size dimension of the extracted patches
    --seg to perform the segmentation
    --patch to generate the 'patches' folder, where the patches coordinate are stored
    --stitch to generate the 'stitches' folder
    --process_list csv file (as the autogenerated one) that indicates the segmentation parameters per 
    each slide and whether it should be processed or not


**_STEP 2: FEATURES EXTRACTION_**

1) Remove filename estensions from RESULTS_DIRECTORY/process_list_autogen.csv

python removeFilenameExtensions.py --path RESULTS_DIRECTORY/process_list_autogen.csv

2) Extract Features


CUDA_VISIBLE_DEVICES=0,1 python extract_features_fp.py --data_h5_dir DIR_TO_COORDS --data_slide_dir DATA_DIRECTORY --csv_path CSV_FILE_NAME --feat_dir FEATURES_DIRECTORY --batch_size 512 --slide_ext .svs

# Our command:
CUDA_VISIBLE_DEVICES=0,1,2,3 python extract_features_fp.py --data_h5_dir ../SEGMENTATION_DIR --data_slide_dir /group/glastonbury/GTEX_WSI_IMAGES --csv_path ../SEGMENTATION_DIR/process_list_autogen.csv_noext.csv --feat_dir ../GTEX_FEATURES --batch_size 256 --slide_ext .svs


**parameters**

    CUDA_VISIBLE_DEVICES selects the IDs of the GPUs to be used
    --data_h5_dir path to directory where the coordinates .h5 files are stored
    --data_slide_dir DATA_DIRECTORY where the slides are stored
    --csv_path csv file as in the segmentation step; it is expected to contain a list of slide filenames 
      (without the filename extensions) to process (the easiest option is to take the csv file auto generated 
       by the previous segmentation/patching step, and delete the filename extensions (removeFilenameExtensions.py))
    --feat_dir FEATURES_DIRECTORY, where the features will be stored; features are 1024-dim
    --batch_size batch size
    --slide ext slides file extension

    FEATURES_DIRECTORY/
         ├── h5_files
                    ├── slide_1.h5
                    ├── slide_2.h5
                    └── ...
         └── pt_files
                    ├── slide_1.pt
                    ├── slide_2.pt
                    └── ...
    
    where each .h5 file contains an array of extracted features along with their patch coordinates 
    (note for faster training, a .pt file for each slide is also created for each slide, containing just 
    the patch features)

**_STEP 3: SPLITS CREATION_**

1. Create a csv file as the ones in /dataset_csv/ with the createCsv script:

python createCsv.py --path GTEX/arterial_subset_recoded.csv --savename GTEX/WSI_gtex.csv  

2. Create splits

python create_splits_seq.py --task wsi_classification --seed 1 --label_frac 1.0 --k 1

    For evaluating the algorithm's performance, multiple folds (e.g. 10-fold) of train/val/test
    splits can be used. Example 10-fold 80/10/10 splits for the two dummy datasets, using 50% 
    of training data can be found under the splits folder. These splits can be automatically 
    generated using the create_splits_seq.py

**parameters**

    --task : task name (both the main.py and the create_splits_seq.py scripts have to be modified
             adding the task to be performed)
    --seed
    --label_frac : fraction of labels for training
    --k : number of splits
    --task : task name
    --val_frac : fraction of labels for validation 
    --test_frac : fraction of labels for test
    

**_STEP 4: MODEL TRAINING_**

CUDA_VISIBLE_DEVICES=0,1 python main.py --drop_out --early_stopping --lr 1e-4 --k 1 --label_frac 1.0 --exp_code atheroexpress_classification_binary_3 --model_size dino_version --bag_loss ce --task wsi_classification_binary --model_type clam_mb --log_data --weighted_sample --subtyping --data_root_dir /hpc/dhl_ec/fcisternino/ATHEROEXPRESS_PROCESSED/features_512


    The data used for training and testing are expected to be organized as follows:
    
    DATA_ROOT_DIR/
        ├──DATASET_1_DATA_DIR/
            ├── h5_files
                    ├── slide_1.h5
                    ├── slide_2.h5
                    └── ...
            └── pt_files
                    ├── slide_1.pt
                    ├── slide_2.pt
                    └── ...
        ├──DATASET_2_DATA_DIR/
            ├── h5_files
                    ├── slide_a.h5
                    ├── slide_b.h5
                    └── ...
            └── pt_files
                    ├── slide_a.pt
                    ├── slide_b.pt
                    └── ...
        └──DATASET_3_DATA_DIR/
            ├── h5_files
                    ├── slide_i.h5
                    ├── slide_ii.h5
                    └── ...
            └── pt_files
                    ├── slide_i.pt
                    ├── slide_ii.pt
                    └── ...
        └── ...
    
    Namely, each dataset is expected to be a subfolder (e.g. DATASET_1_DATA_DIR) under DATA_ROOT_DIR, 
    and the features extracted for each slide in the dataset is stored as a .pt file sitting under the 
    pt_files folder of this subfolder. Datasets are also expected to be prepared in a csv format containing 
    at least 3 columns: case_id, slide_id, and 1 or more labels columns for the slide-level labels. 
    Each case_id is a unique identifier for a patient, while the slide_id is a unique identifier for a slide 
    that correspond to the name of an extracted feature .pt file. This is necessary because often one patient 
    has multiple slides, which might also have different labels. 

    When train/val/test splits are created, we also make sure that slides from the same patient do not go to 
    different splits. The slide ids should be consistent with what was used during the feature extraction step. 
    
    Two examples of such dataset csv files are provided in the dataset_csv folder: one for binary 
    tumor vs. normal classification (task 1) and one for multi-class tumor_subtyping (task 2).
    
    Dataset objects used for actual training/validation/testing can be constructed using the Generic_MIL_Dataset Class 
    (defined in datasets/dataset_generic.py). Examples of such dataset objects passed to the models can be found in 
    both main.py and eval.py.

Main script => main.py
    
**parameters**
    
    --data_root_dir : DATA_ROOT_DIR as explained above
    --max_epochs : maximum number of epochs
    --lr : learning rate
    --label_frac fraction of training labels (train/val/test split)
    --reg : weight decay (regularization)
    --seed : reproducibility (?)
    --k : number of folds
    --k_start : start fold
    --k_end : end fold
    --results_dir : directory where to store the results
    --split_dir
    --log_data : insert this parameter to log data using tensorboard
    --testing : debugging tool (?)
    --early_stopping : enables early stopping
    --opt : optimizer (Adam, SGD, etc.)
    --drop_out : enables dropout with p=0.25
    --bag_loss : slide-level classification loss function
    --model_type : type of model (default: clam_sb (single branch), clam w/ single attention branch)
    --exp_code : Experiment code to save results
    --weighted_sample : enables weighted sampling (typically, classes with a lower frequency on the
                        dataset are given an higher probability of being sampled
    --model_size : size of the model (choises are ['small', 'big']; to be explored)
    --task : binary classification or subtyping

##### CLAM specific options
    --no_inst_cluster : disables instance-level clustering
    --inst_loss : instance-level clustering loss function (default: None)
    --subtyping : insert this parameter if it is a subtyping problem
    --bag_weight : weight for the bag loss (wrt the instance-level clustering loss)
    --B: number of positive/negative patches to sample for CLAM (see paper)


1) Dataset Loading 
   
        Example:
            args.n_classes=3
            dataset = Generic_MIL_Dataset(csv_path = 'dataset_csv/tumor_subtyping_dummy_clean.csv',
                                    data_dir= os.path.join(args.data_root_dir, 'tumor_subtyping_resnet_features'),
                                    shuffle = False, 
                                    seed = args.seed, 
                                    print_info = True,
                                    label_dict = {'subtype_1':0, 'subtype_2':1, 'subtype_3':2},
                                    patient_strat= False,
                                    ignore=[])
        if args.model_type in ['clam_sb', 'clam_mb']:
            assert args.subtyping 

        The dataset builder takes as input a csv file as the one in CLAM/datasets/tumor_subtyping_dummy_clean.csv
        which tells, for each couple (patient, slide) the corresponding label (subtype_1, subtype_2, subtype_3).
        The id of the slide in the .csv file and the id of the same slide in the features folder (DATA_ROOT_DIR)
        have to be the same.
        The dataset class contains the  __getitem__(idx) method that, for a given index, returns the features, the label
        and eventally the coordinates of the patches.
        
2) Model Training
Inside the main function, the 'train' method is called:
        
        results, test_auc, val_auc, test_acc, val_acc  = train(datasets, i, args) 



Considerations: the feature extraction part and the CLAM (or MIL) models training are two independent
and separated parts: the models training, indeed, doesn't take as input the slide, but the extracted 
features.

3) Model Evaluation

The eval.py scripts performs an evaluation of the model

CUDA_VISIBLE_DEVICES=0 python eval.py --drop_out --fold 0 
--models_exp_code wsi_classification_75_1_s1 --save_exp_code wsi_classification_75_s1_ev 
--task wsi_classification --model_type clam_sb --results_dir results --data_root_dir ../GTEX_FEATURES 
--micro_average


srun --nodes=1 --tasks-per-node=1 --time=10:00:00 --partition=gpu --gres=gpu:2 --pty /bin/bash

